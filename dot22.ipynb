{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Dharnish/tamil-dialect-standardization/blob/main/dot22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "WCzJslqmt9_I",
        "outputId": "8e2df735-fbf6-4ddf-fb4b-ecf1313cc79f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9eb53e5-98a0-48ed-ac75-c7d4169b41c1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e9eb53e5-98a0-48ed-ac75-c7d4169b41c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving CIP_DATASETS.xlsx to CIP_DATASETS.xlsx\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Special tokens and indices\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "PAD_IDX, SOS_IDX, EOS_IDX, UNK_IDX = 0, 1, 2, 3\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from time import time\n",
        "\n",
        "# Early Stopping class\n",
        "class EarlyStopping:\n",
        "    def _init_(self, patience=5, delta=0):\n",
        "        self.patience = patience  # how many epochs to wait for improvement\n",
        "        self.delta = delta        # minimum change to qualify as an improvement\n",
        "        self.counter = 0          # how many epochs without improvement\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False   # Flag to stop training if True\n",
        "\n",
        "    def _call_(self, val_loss, model):\n",
        "        # If the validation loss decreases, reset the counter and update the best_loss\n",
        "        if val_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            torch.save(model.state_dict(), \"best_seq2seq_model.pt\")  # Save the model with the best validation loss\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        # If counter exceeds patience, set early_stop to True\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data Preparation\n",
        "# -----------------------------\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "def build_vocab(sentences, min_freq=1):\n",
        "    freq = {}\n",
        "    for sent in sentences:\n",
        "        for word in tokenize(sent):\n",
        "            freq[word] = freq.get(word, 0) + 1\n",
        "\n",
        "    vocab = {PAD_TOKEN: PAD_IDX, SOS_TOKEN: SOS_IDX, EOS_TOKEN: EOS_IDX, UNK_TOKEN: UNK_IDX}\n",
        "    idx = 4\n",
        "    for word, count in freq.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "    return vocab\n",
        "\n",
        "def numericalize(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    return [vocab.get(token, UNK_IDX) for token in tokens]\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def _init_(self, filepath, src_vocab=None, trg_vocab=None):\n",
        "        df = pd.read_excel(filepath)\n",
        "        self.src_sentences = df.iloc[:, 0].astype(str).tolist()\n",
        "        self.trg_sentences = df.iloc[:, 1].astype(str).tolist()\n",
        "\n",
        "        self.src_vocab = build_vocab(self.src_sentences) if src_vocab is None else src_vocab\n",
        "        self.trg_vocab = build_vocab(self.trg_sentences) if trg_vocab is None else trg_vocab\n",
        "    def _getitems_(self, indices):\n",
        "       return [self[i] for i in indices]\n",
        "\n",
        "    def _len_(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def _getitem_(self, index):\n",
        "        src = self.src_sentences[index]\n",
        "        trg = self.trg_sentences[index]\n",
        "\n",
        "        src_ids = [SOS_IDX] + numericalize(src, self.src_vocab) + [EOS_IDX]\n",
        "        trg_ids = [SOS_IDX] + numericalize(trg, self.trg_vocab) + [EOS_IDX]\n",
        "\n",
        "        return torch.tensor(src_ids), torch.tensor(trg_ids)\n",
        "\n",
        "def pad_collate(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_lens = [len(s) for s in src_batch]\n",
        "    trg_lens = [len(t) for t in trg_batch]\n",
        "\n",
        "    src_max_len = max(src_lens)\n",
        "    trg_max_len = max(trg_lens)\n",
        "\n",
        "    padded_src = [torch.cat([s, torch.full((src_max_len - len(s),), PAD_IDX, dtype=torch.long)]) for s in src_batch]\n",
        "    padded_trg = [torch.cat([t, torch.full((trg_max_len - len(t),), PAD_IDX, dtype=torch.long)]) for t in trg_batch]\n",
        "\n",
        "    return torch.stack(padded_src), torch.stack(padded_trg)\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "class Encoder(nn.Module):\n",
        "    def _init_(self, input_dim, emb_dim, hid_dim, num_layers=1, dropout=0.5):\n",
        "        super()._init_()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def _init_(self, output_dim, emb_dim, hid_dim, num_layers=1, dropout=0.5):\n",
        "        super()._init_()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def _init_(self, encoder, decoder, device):\n",
        "        super()._init_()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval\n",
        "# -----------------------------\n",
        "def train(model, dataloader, optimizer, criterion, clip, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in dataloader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, trg_vocab):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    all_preds = []\n",
        "    all_trues = []\n",
        "    bleu_scores = []\n",
        "    inv_trg_vocab = {idx: token for token, idx in trg_vocab.items()}\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg, teacher_forcing_ratio=0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output_tokens = output.argmax(2)\n",
        "            output_flat = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg_flat = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            for pred_seq, true_seq in zip(output_tokens, trg):\n",
        "                pred_words = [inv_trg_vocab.get(tok.item(), UNK_TOKEN) for tok in pred_seq if tok != PAD_IDX]\n",
        "                true_words = [inv_trg_vocab.get(tok.item(), UNK_TOKEN) for tok in true_seq if tok != PAD_IDX]\n",
        "                bleu = sentence_bleu([true_words], pred_words, smoothing_function=smoothie)\n",
        "                bleu_scores.append(bleu)\n",
        "\n",
        "            all_preds.extend(output_flat.argmax(1).cpu().numpy())\n",
        "            all_trues.extend(trg_flat.cpu().numpy())\n",
        "\n",
        "            loss = criterion(output_flat, trg_flat)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    accuracy = accuracy_score(all_trues, all_preds)\n",
        "    precision = precision_score(all_trues, all_preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(all_trues, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_trues, all_preds, average='macro', zero_division=0)\n",
        "    bleu_avg = np.mean(bleu_scores)\n",
        "\n",
        "    return epoch_loss / len(dataloader), accuracy, precision, recall, f1, bleu_avg\n",
        "\n",
        "# -----------------------------\n",
        "# Translate\n",
        "# -----------------------------\n",
        "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
        "    model.eval()\n",
        "    tokens = [SOS_TOKEN] + tokenize(sentence) + [EOS_TOKEN]\n",
        "    src_indexes = [src_vocab.get(token, UNK_IDX) for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indexes = [SOS_IDX]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "            pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    inv_trg_vocab = {idx: token for token, idx in trg_vocab.items()}\n",
        "    translated_tokens = [inv_trg_vocab.get(idx, UNK_TOKEN) for idx in trg_indexes[1:-1]]\n",
        "    return ' '.join(translated_tokens)\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbTM93G8vMmv"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "        self.src_texts = dataframe['Tirunelveli Tamil'].tolist()\n",
        "        self.trg_texts = dataframe['Senthamil'].tolist()\n",
        "\n",
        "        self.src_vocab = self.build_vocab(self.src_texts)\n",
        "        self.trg_vocab = self.build_vocab(self.trg_texts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.encode(self.src_texts[idx], self.src_vocab)\n",
        "        trg = self.encode(self.trg_texts[idx], self.trg_vocab)\n",
        "        return torch.tensor(src), torch.tensor(trg)\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        idx = 4\n",
        "        for text in texts:\n",
        "            for token in text.split():\n",
        "                if token not in vocab:\n",
        "                    vocab[token] = idx\n",
        "                    idx += 1\n",
        "        return vocab\n",
        "\n",
        "    def encode(self, text, vocab):\n",
        "        tokens = text.split()\n",
        "        encoded = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "        return [vocab['<sos>']] + encoded + [vocab['<eos>']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ZoudYO-fuw4l",
        "outputId": "131b48f2-8578-4bed-90c5-3fadaba372a7"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Encoder.__init__() takes 1 positional argument but 6 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-905262499a67>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# Run main and get the important objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m val_loader = DataLoader(\n\u001b[1;32m    136\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-905262499a67>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Initialize model, optimizer, and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;34m\" given\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Encoder.__init__() takes 1 positional argument but 6 were given"
          ]
        }
      ],
      "source": [
        "# ==== Configuration ====\n",
        "# ==== Configuration ====\n",
        "BATCH_SIZE = 32\n",
        "EMBED_SIZE = 256\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 80\n",
        "PAD_IDX = 0  # will be updated after vocab build\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    df = pd.read_excel('CIP_DATASETS.xlsx')\n",
        "    df=df.dropna()\n",
        "    if df.isnull().values.any():\n",
        "        raise ValueError(\"CSV file contains NaN values.\")\n",
        "\n",
        "    dataset = TranslationDataset(df)\n",
        "    global PAD_IDX\n",
        "    PAD_IDX = dataset.src_vocab[\"<pad>\"]\n",
        "\n",
        "    # Train, validation, and test split\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = int(0.1 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate)\n",
        "\n",
        "    # Initialize model, optimizer, and loss\n",
        "    encoder = Encoder(len(dataset.src_vocab), EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "    decoder = Decoder(len(dataset.trg_vocab), EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "    model = Seq2Seq(encoder, decoder, PAD_IDX).to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, DEVICE)\n",
        "        val_loss, val_acc, val_prec, val_rec, val_f1, val_bleu = evaluate(model, val_loader, criterion, DEVICE, dataset.trg_vocab)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
        "        print(f\"Train Loss   : {train_loss:.3f}\")\n",
        "        print(f\"Val Loss     : {val_loss:.3f}\")\n",
        "        print(f\"Accuracy     : {val_acc:.4f}\")\n",
        "        print(f\"Precision    : {val_prec:.4f}\")\n",
        "        print(f\"Recall       : {val_rec:.4f}\")\n",
        "        print(f\"F1 Score     : {val_f1:.4f}\")\n",
        "        print(f\"BLEU Score   : {val_bleu:.4f}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    test_loss, test_acc, test_prec, test_rec, test_f1, test_bleu = evaluate(\n",
        "        model, test_loader, criterion, DEVICE, dataset.trg_vocab\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Test Set Evaluation ---\")\n",
        "    print(f\"Test Loss    : {test_loss:.3f}\")\n",
        "    print(f\"Accuracy     : {test_acc:.4f}\")\n",
        "    print(f\"Precision    : {test_prec:.4f}\")\n",
        "    print(f\"Recall       : {test_rec:.4f}\")\n",
        "    print(f\"F1 Score     : {test_f1:.4f}\")\n",
        "    print(f\"BLEU Score   : {test_bleu:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix (optional: can be val_loader or test_loader)\n",
        "    plot_confusion_matrix(model, test_loader, dataset.trg_vocab, DEVICE, pad_idx=PAD_IDX)\n",
        "\n",
        "    return model, dataset, DEVICE, test_loader\n",
        "\n",
        "# -------------------------\n",
        "# Plotting Section\n",
        "# -------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def plot_confusion_matrix(model, val_loader, trg_vocab, device, pad_idx=0, max_tokens=30):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_trues = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, trg_batch in val_loader:\n",
        "            src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n",
        "            output = model(src_batch, trg_batch[:, :-1])  # remove <eos> from input\n",
        "            output_tokens = output.argmax(dim=-1)  # predicted token indices\n",
        "\n",
        "            true_tokens = trg_batch[:, 1:]  # shift to match output\n",
        "\n",
        "            for pred_seq, true_seq in zip(output_tokens, true_tokens):\n",
        "                for pred_token, true_token in zip(pred_seq, true_seq):\n",
        "                    if true_token.item() != pad_idx:\n",
        "                        all_preds.append(pred_token.item())\n",
        "                        all_trues.append(true_token.item())\n",
        "\n",
        "    # Convert vocab indices to tokens for readable labels\n",
        "    idx_to_token = {v: k for k, v in trg_vocab.items()}\n",
        "\n",
        "    # Limit labels to top-N most common tokens\n",
        "    labels = list(set(all_trues + all_preds))\n",
        "    if len(labels) > max_tokens:\n",
        "        labels = labels[:max_tokens]  # truncate to top N\n",
        "\n",
        "    cm = confusion_matrix(all_trues, all_preds, labels=labels, normalize='true')\n",
        "\n",
        "    label_names = [idx_to_token[i] for i in labels]\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    disp.plot(include_values=False, cmap='Blues', ax=ax, xticks_rotation='vertical')\n",
        "    plt.title(\"Token-level Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run main and get the important objects\n",
        "model, dataset, device = main()\n",
        "val_loader = DataLoader(\n",
        "    torch.utils.data.Subset(dataset, range(int(0.9 * len(dataset)), len(dataset))),\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    collate_fn=pad_collate\n",
        ")\n",
        "\n",
        "plot_confusion_matrix(model, val_loader, dataset.trg_vocab, device, pad_idx=PAD_IDX)\n",
        "# Now define your translation function\n",
        "def trans(example):\n",
        "    translation = translate_sentence(example, dataset.src_vocab, dataset.trg_vocab, model, device)\n",
        "\n",
        "\n",
        "    return translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1kFGZDHwshS",
        "outputId": "16d1caf6-eb0f-43be-d164-2b4338857f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxvQFAcnwMGH",
        "outputId": "a9a9e2b4-ff99-4d2c-bcc6-ca66a6b9af2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 4.9021\n",
            "Epoch 2/50, Loss: 3.8066\n",
            "Epoch 3/50, Loss: 3.7547\n",
            "Epoch 4/50, Loss: 3.7296\n",
            "Epoch 5/50, Loss: 3.7104\n",
            "Epoch 6/50, Loss: 3.6738\n",
            "Epoch 7/50, Loss: 3.6424\n",
            "Epoch 8/50, Loss: 3.6090\n",
            "Epoch 9/50, Loss: 3.5747\n",
            "Epoch 10/50, Loss: 3.5322\n",
            "Epoch 11/50, Loss: 3.4902\n",
            "Epoch 12/50, Loss: 3.4550\n",
            "Epoch 13/50, Loss: 3.4242\n",
            "Epoch 14/50, Loss: 3.3932\n",
            "Epoch 15/50, Loss: 3.3672\n",
            "Epoch 16/50, Loss: 3.3453\n",
            "Epoch 17/50, Loss: 3.3178\n",
            "Epoch 18/50, Loss: 3.2933\n",
            "Epoch 19/50, Loss: 3.2714\n",
            "Epoch 20/50, Loss: 3.2460\n",
            "Epoch 21/50, Loss: 3.2232\n",
            "Epoch 22/50, Loss: 3.1964\n",
            "Epoch 23/50, Loss: 3.1680\n",
            "Epoch 24/50, Loss: 3.1404\n",
            "Epoch 25/50, Loss: 3.1175\n",
            "Epoch 26/50, Loss: 3.0935\n",
            "Epoch 27/50, Loss: 3.0706\n",
            "Epoch 28/50, Loss: 3.0438\n",
            "Epoch 29/50, Loss: 3.0165\n",
            "Epoch 30/50, Loss: 2.9909\n",
            "Epoch 31/50, Loss: 2.9672\n",
            "Epoch 32/50, Loss: 2.9407\n",
            "Epoch 33/50, Loss: 2.9225\n",
            "Epoch 34/50, Loss: 2.8979\n",
            "Epoch 35/50, Loss: 2.8710\n",
            "Epoch 36/50, Loss: 2.8456\n",
            "Epoch 37/50, Loss: 2.8257\n",
            "Epoch 38/50, Loss: 2.8072\n",
            "Epoch 39/50, Loss: 2.7864\n",
            "Epoch 40/50, Loss: 2.7598\n",
            "Epoch 41/50, Loss: 2.7361\n",
            "Epoch 42/50, Loss: 2.7216\n",
            "Epoch 43/50, Loss: 2.6902\n",
            "Epoch 44/50, Loss: 2.6642\n",
            "Epoch 45/50, Loss: 2.6488\n",
            "Epoch 46/50, Loss: 2.6291\n",
            "Epoch 47/50, Loss: 2.6110\n",
            "Epoch 48/50, Loss: 2.5867\n",
            "Epoch 49/50, Loss: 2.5667\n",
            "Epoch 50/50, Loss: 2.5423\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# ==== Configuration ====\n",
        "BATCH_SIZE = 128\n",
        "EMBED_SIZE = 512\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 5\n",
        "DROPOUT = 0.3\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 80\n",
        "PAD_IDX = 0  # placeholder\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==== Tokenization ====\n",
        "def tokenize(text):\n",
        "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "# ==== Dataset ====\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.pairs = list(zip(df['Tirunelveli Tamil'], df['Senthamil']))\n",
        "        self.src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        self.trg_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        self.src_inv_vocab = {}\n",
        "        self.trg_inv_vocab = {}\n",
        "        self.build_vocab()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        src_counter = Counter()\n",
        "        trg_counter = Counter()\n",
        "        for src, trg in self.pairs:\n",
        "            src_counter.update(tokenize(src))\n",
        "            trg_counter.update(tokenize(trg))\n",
        "        for word in src_counter:\n",
        "            if word not in self.src_vocab:\n",
        "                self.src_vocab[word] = len(self.src_vocab)\n",
        "        for word in trg_counter:\n",
        "            if word not in self.trg_vocab:\n",
        "                self.trg_vocab[word] = len(self.trg_vocab)\n",
        "        self.src_inv_vocab = {i: w for w, i in self.src_vocab.items()}\n",
        "        self.trg_inv_vocab = {i: w for w, i in self.trg_vocab.items()}\n",
        "\n",
        "    def encode(self, sentence, vocab):\n",
        "        tokens = tokenize(sentence)\n",
        "        return [vocab.get(tok, vocab['<unk>']) for tok in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, trg = self.pairs[idx]\n",
        "        src_ids = [self.src_vocab['<sos>']] + self.encode(src, self.src_vocab) + [self.src_vocab['<eos>']]\n",
        "        trg_ids = [self.trg_vocab['<sos>']] + self.encode(trg, self.trg_vocab) + [self.trg_vocab['<eos>']]\n",
        "        return torch.tensor(src_ids), torch.tensor(trg_ids)\n",
        "\n",
        "# ==== Padding ====\n",
        "def pad_collate(batch):\n",
        "    srcs, trgs = zip(*batch)\n",
        "    srcs_padded = nn.utils.rnn.pad_sequence(srcs, batch_first=True, padding_value=PAD_IDX)\n",
        "    trgs_padded = nn.utils.rnn.pad_sequence(trgs, batch_first=True, padding_value=PAD_IDX)\n",
        "    return srcs_padded, trgs_padded\n",
        "\n",
        "# ==== Encoder ====\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_size, hidden_size, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embed_size, padding_idx=PAD_IDX)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "# ==== Decoder ====\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_size, hidden_size, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, embed_size, padding_idx=PAD_IDX)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.unsqueeze(1)  # batch_size -> (batch, 1)\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        return prediction, hidden\n",
        "\n",
        "# ==== Seq2Seq ====\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        batch_size, trg_len = trg.shape\n",
        "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(DEVICE)\n",
        "        hidden = self.encoder(src)\n",
        "        input = trg[:, 0]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            outputs[:, t] = output\n",
        "            input = output.argmax(1)\n",
        "        return outputs\n",
        "\n",
        "# ==== Training ====\n",
        "def train(model, data_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg in data_loader:\n",
        "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(data_loader)\n",
        "\n",
        "# ==== Main ====\n",
        "def main():\n",
        "    df = pd.read_excel(\"CIP_DATASETS.xlsx\")\n",
        "    df=df.dropna()\n",
        "    if df.isnull().any().any():\n",
        "        raise ValueError(\"CSV file contains NaN values.\")\n",
        "    dataset = TranslationDataset(df)\n",
        "\n",
        "    global PAD_IDX\n",
        "    PAD_IDX = dataset.src_vocab[\"<pad>\"]\n",
        "\n",
        "    # Split\n",
        "    total_len = len(dataset)\n",
        "    train_end = int(0.8 * total_len)\n",
        "    val_end = int(0.9 * total_len)\n",
        "    train_dataset = torch.utils.data.Subset(dataset, range(train_end))\n",
        "    val_dataset = torch.utils.data.Subset(dataset, range(train_end, val_end))\n",
        "    test_dataset = torch.utils.data.Subset(dataset, range(val_end, total_len))\n",
        "\n",
        "    # Loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate)\n",
        "\n",
        "    # Model\n",
        "    encoder = Encoder(len(dataset.src_vocab), EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "    decoder = Decoder(len(dataset.trg_vocab), EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "    model = Seq2Seq(encoder, decoder, PAD_IDX).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss = train(model, train_loader, optimizer, criterion)\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss:.4f}\")\n",
        "\n",
        "    return model, dataset, DEVICE\n",
        "\n",
        "# ==== Run ====\n",
        "model, dataset, device = main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fox5BxttxyJh",
        "outputId": "86a85424-266d-4242-b1c7-af46e4278942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.1856\n",
            "BLEU Score: 0.0510\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate(model, dataset, device):\n",
        "    test_dataset = torch.utils.data.Subset(dataset, range(int(0.9 * len(dataset)), len(dataset)))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=pad_collate)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_trues = []\n",
        "    bleu_scores = []\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in test_loader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg)\n",
        "            pred_ids = output.argmax(dim=-1).squeeze().tolist()\n",
        "            true_ids = trg.squeeze().tolist()\n",
        "\n",
        "            # Remove special tokens for metric calculation\n",
        "            pred_tokens = [dataset.trg_inv_vocab.get(i, \"\") for i in pred_ids if i not in [PAD_IDX, dataset.trg_vocab['<sos>'], dataset.trg_vocab['<eos>']]]\n",
        "            true_tokens = [dataset.trg_inv_vocab.get(i, \"\") for i in true_ids if i not in [PAD_IDX, dataset.trg_vocab['<sos>'], dataset.trg_vocab['<eos>']]]\n",
        "\n",
        "            # For accuracy\n",
        "            min_len = min(len(pred_tokens), len(true_tokens))\n",
        "            correct_tokens += sum(p == t for p, t in zip(pred_tokens[:min_len], true_tokens[:min_len]))\n",
        "            total_tokens += min_len\n",
        "\n",
        "            # For BLEU\n",
        "            smoothie = SmoothingFunction().method4\n",
        "            bleu = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "            bleu_scores.append(bleu)\n",
        "\n",
        "            # For F1\n",
        "            all_preds.extend(pred_tokens)\n",
        "            all_trues.extend(true_tokens)\n",
        "\n",
        "    accuracy = correct_tokens / total_tokens if total_tokens else 0\n",
        "    average_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "\n",
        "\n",
        "    print(f\"\\nEvaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"BLEU Score: {average_bleu:.4f}\")\n",
        "\n",
        "\n",
        "# ==== Run Evaluation ====\n",
        "evaluate(model, dataset, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA95QkZHkkqd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}